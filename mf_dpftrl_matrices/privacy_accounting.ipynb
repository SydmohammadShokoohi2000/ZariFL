{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP96g72HnUKz"
      },
      "source": [
        "This colab details the privacy accounting of a few models trained with Matrix-Factorization-Differentially-Private-Follow-The-Regularized-Leader (MF-DP-FTRL) by [(Amplified) Banded Matrix Factorization: A unified approach to private training](https://arxiv.org/abs/2306.08153) in cross-device federated learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Z9IEAqpZgd"
      },
      "source": [
        "# Preliminaries\n",
        "\n",
        "Install and import packages including tensorflow and dp_accounting. The dp_accounting library will only be used for the [zCDP](https://arxiv.org/abs/1605.02065) to $(\\epsilon, \\delta)-$DP conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDHUxiucpWD4"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "! pip install dp-accounting==0.4.3\n",
        "import dp_accounting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d3_cCZSZ_iX"
      },
      "source": [
        "## Downloads matrices $B, C$ for MF-DP-FTRL.\n",
        "\n",
        "Recall that we use the inverse of matrix $C$ to generate round-correlated noise, and for training round $i$, we have\n",
        "$\\hat{x}_i \\gets x_i + \\zeta [C^{-1}z]_{[i,:]}$, where $x_i$ is a $d-$dimentional vector represents the aggregated signal of client updates; $\\zeta$ is a scalar of the $L_2$ clipping norm for each client update to control sensitivity; $z$ is the $(n,d)$-dimentional matrix of Gaussian noise with zero mean and standard deviation $\\sigma$, and $n$ is the total number of rounds. The $i$th row of the correlated noise $[C^{-1}z]_{[i,:]}$ is applied to privatirized the update $x_i$ in the $i$th round. We then use the privatirized update $\\hat{x}_i$ to update the global model.\n",
        "\n",
        "In addition to providing the $C$ matrix in advance, the users would also need to specify hyperparameters $\\zeta$ for $L_2$ clipping norm of client updates and $\\sigma$ for noise multiplier w.r.t the $L_2$ clip norm, when run the MF-DP-FTRL algorithm for training, and $\\sigma$ has significant impact on the privacy guarantees in accounting.\n",
        "\n",
        "$B, C$ matrices are generated by optimizing an objective of expected total squared error given the contraint that $A=BC$ where $A$ is a $(n, n)-$ dimentional lower trianglar with $A_{[i,j]}= 1, \\forall i\\leq j$, $A_{[i,j]}= 0, \\forall i \u003e j$ and $C$ is a $(n, n)-$ dimentional matrix of both lower trianglar and $\\hat{b}$-banded with $C_{[i,j]}= 0, \\forall i \u003e j$, and $C_{[i,j]}=0, \\forall |i-j| \\geq \\hat{b}$. See [(Amplified) Banded Matrix Factorization: A unified approach to private training](https://arxiv.org/abs/2306.08153) for more details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 54,
          "status": "ok",
          "timestamp": 1708034879232,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "v8-HfW2E2uvf",
        "outputId": "561d2567-d47e-4dae-ad6a-f08602da615a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data location:\n",
            " https://github.com/google-research/federated/raw/master/mf_dpftrl_matrices/bands400_rounds2000/b_matrix_tensor_pb\n",
            " https://github.com/google-research/federated/raw/master/mf_dpftrl_matrices/bands400_rounds2000/c_matrix_tensor_pb\n",
            " https://github.com/google-research/federated/raw/master/mf_dpftrl_matrices/bands400_rounds2000/lr_vector_tensor_pb\n",
            " https://github.com/google-research/federated/raw/master/mf_dpftrl_matrices/bands1000_rounds2000/b_matrix_tensor_pb\n",
            " https://github.com/google-research/federated/raw/master/mf_dpftrl_matrices/bands1000_rounds2000/c_matrix_tensor_pb\n"
          ]
        }
      ],
      "source": [
        "data_location = 'https://github.com/google-research/federated/raw/master/mf_dpftrl_matrices/{}'\n",
        "b1_url = data_location.format('bands400_rounds2000/b_matrix_tensor_pb')\n",
        "c1_url = data_location.format('bands400_rounds2000/c_matrix_tensor_pb')\n",
        "lr1_url = data_location.format('bands400_rounds2000/lr_vector_tensor_pb')\n",
        "\n",
        "b2_url = data_location.format('bands1000_rounds2000/b_matrix_tensor_pb')\n",
        "c2_url = data_location.format('bands1000_rounds2000/c_matrix_tensor_pb')\n",
        "print(f'Data location:\\n {b1_url}\\n {c1_url}\\n {lr1_url}\\n {b2_url}\\n {c2_url}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAJSHWabZrQU"
      },
      "outputs": [],
      "source": [
        "b1_file = tf.keras.utils.get_file(fname='b1', origin=b1_url)\n",
        "c1_file = tf.keras.utils.get_file(fname='c1', origin=c1_url)\n",
        "lr1_file = tf.keras.utils.get_file(fname='lr1', origin=lr1_url)\n",
        "\n",
        "b2_file = tf.keras.utils.get_file(fname='b2', origin=b2_url)\n",
        "c2_file = tf.keras.utils.get_file(fname='c2', origin=c2_url)\n",
        "print(\n",
        "    f'Local data location:\\n {b1_file}\\n {c1_file}\\n {lr1_file}\\n {b2_file}\\n'\n",
        "    f' {c2_file}'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm0XucapSUzi"
      },
      "outputs": [],
      "source": [
        "def load_matrix(file_path: str) -\u003e tf.Tensor:\n",
        "  return tf.io.parse_tensor(tf.io.read_file(file_path), tf.float64)\n",
        "\n",
        "\n",
        "b1_matrix = load_matrix(b1_file)\n",
        "c1_matrix = load_matrix(c1_file)\n",
        "lr1_vector = load_matrix(lr1_file)\n",
        "\n",
        "b2_matrix = load_matrix(b2_file)\n",
        "c2_matrix = load_matrix(c2_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avk_t2pDtmBY"
      },
      "source": [
        "## Verify and visualize the matrices\n",
        "\n",
        "We have loaded two set of matrices $(B1, C1)$ with a learning rate decaying vector $LR1$ for total 2000 training rounds with 400 bands; and $(B2, C2)$ for total 2000 trainig rounds with 1000 bands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngH6_WxrtsIk"
      },
      "outputs": [],
      "source": [
        "def verify_and_visualize(\n",
        "    b_matrix: tf.Tensor,\n",
        "    c_matrix: tf.Tensor,\n",
        "    bands: int,\n",
        "    lr_vector: tf.Tensor | None = None,\n",
        ") -\u003e None:\n",
        "\n",
        "  if not verify_lower_trianglar_and_banded(c_matrix.numpy(), bands):\n",
        "    raise ValueError(f'C matrix has to be lower trianglar and {bands} bands.')\n",
        "\n",
        "  a_matrix = b_matrix @ c_matrix\n",
        "  if lr_vector is not None:\n",
        "    a_matrix = tf.math.divide_no_nan(\n",
        "        a_matrix, tf.broadcast_to(lr_vector, tf.shape(a_matrix))\n",
        "    )\n",
        "  if not verify_lower_trianglar_ones(a_matrix.numpy()):\n",
        "    raise ValueError(f'A matrix has to be lower trianglar with 1s.')\n",
        "\n",
        "  c_inv = np.linalg.inv(c_matrix.numpy())\n",
        "\n",
        "  fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16, 5))\n",
        "  stats = statistics(b_matrix.numpy())\n",
        "  print('B matrix value (mean, std, min, max):', stats)\n",
        "  b_im = axs[0].matshow(\n",
        "      b_matrix.numpy(),\n",
        "      vmin=max(stats[2], stats[0] - stats[1] * 2),\n",
        "      vmax=min(stats[3], stats[0] + stats[1] * 2),\n",
        "  )\n",
        "  axs[0].title.set_text('B matrix')\n",
        "  fig.colorbar(b_im, shrink=0.5)\n",
        "\n",
        "  stats = statistics(c_matrix.numpy())\n",
        "  print('C matrix value (mean, std, min, max):', stats)\n",
        "  c_im = axs[1].matshow(\n",
        "      c_matrix.numpy(),\n",
        "      vmin=max(stats[2], stats[0] - stats[1] * 2),\n",
        "      vmax=min(stats[3], stats[0] + stats[1] * 2),\n",
        "  )\n",
        "  axs[1].title.set_text('C matrix')\n",
        "  fig.colorbar(c_im, shrink=0.5)\n",
        "\n",
        "  stats = statistics(c_inv)\n",
        "  print('C inverse matrix value (mean, std, min, max):', stats)\n",
        "  c_inv_im = axs[2].matshow(\n",
        "      c_inv,\n",
        "      vmin=max(stats[2], stats[0] - stats[1] * 2),\n",
        "      vmax=min(stats[3], stats[0] + stats[1] * 2),\n",
        "  )\n",
        "  axs[2].title.set_text('C inverse matrix')\n",
        "  fig.colorbar(c_inv_im, shrink=0.5)\n",
        "\n",
        "  a_im = axs[3].matshow(a_matrix.numpy(), vmin=0, vmax=1)\n",
        "  axs[3].title.set_text('A matrix')\n",
        "  fig.colorbar(a_im, shrink=0.5)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def verify_lower_trianglar_and_banded(mat: np.ndarray, bands: int) -\u003e bool:\n",
        "  if not np.allclose(mat, np.tril(mat)):\n",
        "    return False\n",
        "  for i in range(mat.shape[0]):\n",
        "    for j in range(i + 1):\n",
        "      if (i - j) \u003e= bands and mat[i, j] != 0:\n",
        "        return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def verify_lower_trianglar_ones(mat: np.ndarray) -\u003e bool:\n",
        "  return np.allclose(mat, np.tril(np.ones_like(mat)))\n",
        "\n",
        "\n",
        "def statistics(mat: np.ndarray) -\u003e tuple[float, float, float, float]:\n",
        "  mean = np.mean(mat)\n",
        "  std = np.std(mat)\n",
        "  return mean, std, np.min(mat), np.max(mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwpBxhNdy6rb"
      },
      "outputs": [],
      "source": [
        "print('Loaded matrices for 400 bands and 2000 rounds.')\n",
        "verify_and_visualize(b1_matrix, c1_matrix, bands=400, lr_vector=lr1_vector)\n",
        "print('Loaded matrices for 1000 bands and 2000 rounds.')\n",
        "verify_and_visualize(b2_matrix, c2_matrix, bands=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC5x5q51A0k4"
      },
      "source": [
        "# Privacy accounting for MF-DP-FTRL\n",
        "\n",
        "Now we show the privacy accounting for DP models trained with MF-DP-FTRL. We consider the production setting in [Federated Learning of Gboard Language Models with Differential Privacy](https://arxiv.org/abs/2305.18465) with multiple participation and no amplification by sampling.\n",
        "\n",
        "Recall the DP process of $\\hat{x} \\gets x + \\zeta C^{-1}z$, which is equivalent to $A\\hat{x} \\gets B(Cx + \\zeta z)$ with $A, B$ matrices for post-processing. Note that the objective in [(Amplified) Banded Matrix Factorization: A unified approach to private training](https://arxiv.org/abs/2306.08153) for facorizing $B,C$ is independent to the data $x$.\n",
        "\n",
        "The key idea of privacy accounting is to look at the DP release of $Cx + \\zeta z$, and compute the sensitivity of $Cx$ to calibrate with the Gaussian noise $z$ of zero mean and $\\sigma$ standard deviation. We use the neighboring dataset definition in [Practical and Private (Deep) Learning without Sampling or Shuffling\n",
        "](https://arxiv.org/abs/2103.00039): the two neighboring dataset $(x, x')$ differs by only zeroing out the updates of one participating device. And we compute the worst-case sensitivity of $\\| \\max_{x, x'} C (x-x')\\|_F$. One important property we leverage is: the adaptive DP guarantee of streaming matrix mechanisms (i.e., lower triangular matrices) can be directly inferred from their non-adaptive DP guarantees, as discussed in [Improved Differential Privacy for SGD via Optimal Private Linear Operators on Adaptive Streams](https://arxiv.org/abs/2202.08312).\n",
        "\n",
        "If each client device will only participate once, then $(x, x')$ only have difference in one row, and the sensitivity can be computed by $\\max_{x, x'} \\| C (x-x')\\|_F = \\zeta \\max_{j} \\| C_{[:,j]} \\|$ as $\\zeta$ is the $L_2$ clipping norm for each client update.\n",
        "\n",
        "In cross-device federated learning, a device can participate multiple times. However, we also use a policy to restrict the frequency of client device participation. Specifically, a timer on each client is locally checked so that the client is only eligible for training in this round after a specified period of time since the client's previous training round. The policy results in two important parameters for privacy accounting\n",
        " -  *max participation*: the maximum number of times each client may participate in the total number of training rounds\n",
        " - *min separation b*: the minimum number of rounds between two participation of each client. Note that we use the definition in [Practical and Private (Deep) Learning without Sampling or Shuffling\n",
        "](https://arxiv.org/abs/2103.00039), which is off by one compared to [(Amplified) Banded Matrix Factorization: A unified approach to private training](https://arxiv.org/abs/2306.08153). For a client participate in round $r_1$ and round $r_2$, the min separation $b=r_2-r_1-1$.\n",
        "\n",
        "When $C$ is banded with $\\hat b \\leq b+1$, the computation of sensitivity for multiple participation of  is significantly simplified as $\\max_{x, x'} \\| C (x-x')\\|_F  = \\zeta \\max_{\\Pi} \\sqrt{\\sum_{j \\in \\Pi} \\| C_{[:,j]} \\|^2}$, where $\\Pi$ represents the participation pattern of each client. The sensitivity can be efficiently computed by dynamic programming.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_bRXPX2HAUO"
      },
      "outputs": [],
      "source": [
        "def square_sensitivity_c_banded(\n",
        "    c_matrix: np.ndarray, min_separation: int, max_participation: int\n",
        ") -\u003e float:\n",
        "  \"\"\"Computes sensitivity when C is banded and min separation is large.\n",
        "\n",
        "  This is a recursive version of Algorithm 5 in\n",
        "  [(Amplified) Banded Matrix Factorization: A unified approach to private\n",
        "  training](https://arxiv.org/abs/2306.08153).\n",
        "\n",
        "  Arguments:\n",
        "    c_matrix: The C matrix used in the MF-DP-FTRL algorithm.\n",
        "    min_separation: Minimum separation between two adjacent participation of\n",
        "      each client. If a client participates in round $r_1$ and round $r_2$, the\n",
        "      min separation is r_2-r_1-1.\n",
        "    max_participations: The possible maximum number of participations of each\n",
        "      client.\n",
        "\n",
        "  Returns:\n",
        "    The squared sensitivity.\n",
        "  \"\"\"\n",
        "  total_rounds = c_matrix.shape[1]\n",
        "\n",
        "  def _check_possible_participation(num_participation: int, start: int) -\u003e bool:\n",
        "    \"\"\"Checks if the number of participation is possible.\n",
        "\n",
        "    The participatoins are in rounds [start, total_rounds) with min_separation.\n",
        "    \"\"\"\n",
        "    return start + (min_separation + 1) * (num_participation - 1) \u003c total_rounds\n",
        "\n",
        "  while not _check_possible_participation(max_participation, start=0):\n",
        "    print(\n",
        "        f'Warning: the max participation {max_participation} is large for '\n",
        "        f'{total_rounds} rounds with min separation {min_separation}. '\n",
        "        f'Reducing the max participation to {max_participation-1}.'\n",
        "    )\n",
        "    max_participation -= 1\n",
        "\n",
        "  c_col_sq = np.sum(c_matrix**2, axis=0)\n",
        "\n",
        "  @functools.lru_cache(maxsize=None)\n",
        "  def _sensitivity_c_recur(num_participation: int, start: int) -\u003e float:\n",
        "    \"\"\"Recursively computes the squared sensitivity for num_participation.\n",
        "\n",
        "    Considers rounds [start, total_rounds), i.e., c_col_sq[start:].\n",
        "    \"\"\"\n",
        "    if num_participation == 0:\n",
        "      return 0.0\n",
        "    elif not _check_possible_participation(num_participation, start):\n",
        "      return -np.inf\n",
        "    else:\n",
        "      return max(\n",
        "          c_col_sq[start]\n",
        "          + _sensitivity_c_recur(\n",
        "              num_participation - 1, start + min_separation + 1\n",
        "          ),\n",
        "          _sensitivity_c_recur(num_participation, start + 1),\n",
        "      )\n",
        "\n",
        "  return _sensitivity_c_recur(max_participation, 0)\n",
        "\n",
        "\n",
        "def privacy_accounting(\n",
        "    c_matrix: np.ndarray,\n",
        "    min_seperation: int,\n",
        "    max_participation: int,\n",
        "    l2_clip_norm_noise_multiplier: float,\n",
        "    target_delta: float | None = None,\n",
        ") -\u003e tuple[float, float | None]:\n",
        "  \"\"\"Computes DP guarantees for MF-DP-FTRL.\n",
        "\n",
        "  The total sensitivity comes from the l2_clip_norm and the matrix factorization\n",
        "  mechanism, and takes into account the C matrix. The effective\n",
        "  `total_noise_multiplier` w.r.t. the total sensitivity is used for privacy\n",
        "  accounting of Gaussian mechanism. Both Zero-Concentrated Differential Privacy\n",
        "  (zCDP) and (epsilon, delta)-DP guarantees can be computed.\n",
        "\n",
        "  Arguments:\n",
        "    c_matrix: The C matrix used in the MF-DP-FTRL algorithm.\n",
        "    min_separation: Minimum separation between two adjacent participation of\n",
        "      each client. If a client participates in round $r_1$ and round $r_2$, the\n",
        "      min separation is r_2-r_1-1.\n",
        "    max_participations: The possible maximum number of participations of each\n",
        "      client.\n",
        "    l2_clip_norm_noise_multiplier: The noise multiplier w.r.t. the L2 clipping\n",
        "      norm used when running the MF-DP-FTRL algorithm for model training.\n",
        "    target_delta: The target delta used for (epsilon, delta)-DP. If None, no\n",
        "      (epsilon, delta)-DP guarantee is computed, and only zCDP is returned.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of values for two kinds of DP guarantees\n",
        "      (zCDP, epsilon with `target_delta`)\n",
        "      or a scalar of zCDP if `target_delta` is None.\n",
        "  \"\"\"\n",
        "  sens_sq = square_sensitivity_c_banded(\n",
        "      c_matrix, min_seperation, max_participation\n",
        "  )\n",
        "  total_noise_multiplier = l2_clip_norm_noise_multiplier / sens_sq**0.5\n",
        "\n",
        "  zcdp = 1 / (2 * total_noise_multiplier**2)\n",
        "\n",
        "  if target_delta is not None:\n",
        "    # PLD accounting from https://github.com/google/differential-privacy is used\n",
        "    # for (epsilon, delta)-DP.\n",
        "    accountant = dp_accounting.pld.PLDAccountant()\n",
        "    accountant.compose(dp_accounting.GaussianDpEvent(total_noise_multiplier), 1)\n",
        "    eps = accountant.get_epsilon(target_delta)\n",
        "    return zcdp, eps\n",
        "  else:\n",
        "    return zcdp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06e-2QNfmkOx"
      },
      "source": [
        "## Gboad es-ES NWP LM\n",
        "\n",
        "The Gboard Spanish model in Spain is trained with the loaded matrix $C_1$ of 2000 rounds and 400 bounds, noise multiplier 1.411; the min separation is 484, and the max participation is 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 185,
          "status": "ok",
          "timestamp": 1708225751762,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "1gPPr-GqAhxv",
        "outputId": "5e31abe2-8e85-4962-ec9e-6bcfd9059c00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DP guarantees: zcdp = 0.1506840301548885, (eps, delta) = (3.4240074094281336, 1e-10)\n"
          ]
        }
      ],
      "source": [
        "sys.setrecursionlimit(2000 * 3)  # total_rounds * max_participation\n",
        "zcdp, eps = privacy_accounting(\n",
        "    c1_matrix,\n",
        "    min_seperation=484,\n",
        "    max_participation=3,\n",
        "    l2_clip_norm_noise_multiplier=1.411,\n",
        "    target_delta=1e-10\n",
        ")\n",
        "print(f'DP guarantees: zcdp = {zcdp}, (eps, delta) = ({eps}, 1e-10)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Fn_OKltz2h"
      },
      "source": [
        "## Gboad pt-BR NWP LM\n",
        "\n",
        " The Portuguese model in Brazil is trained with the loaded matrix of $C_2$ of 2000 rounds and 1000 bands, noise multiplier of 8.35; the min separation is 1428 and the max participation is 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 116,
          "status": "ok",
          "timestamp": 1708225755372,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "Fdj_i6HIwPfb",
        "outputId": "44861510-6bd2-42c8-e70e-f9dbfb97418e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DP guarantees: zcdp = 0.014342572340349278, (eps, delta) = (0.9935500950539097, 1e-10)\n"
          ]
        }
      ],
      "source": [
        "sys.setrecursionlimit(2000 * 2)  # total_rounds * max_participation\n",
        "zcdp, eps = privacy_accounting(\n",
        "    c2_matrix,\n",
        "    min_seperation=1428,\n",
        "    max_participation=2,\n",
        "    l2_clip_norm_noise_multiplier=8.35,\n",
        "    target_delta=1e-10\n",
        ")\n",
        "print(f'DP guarantees: zcdp = {zcdp}, (eps, delta) = ({eps}, 1e-10)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CG7_0r31jqx"
      },
      "source": [
        "## Gboad es-US NWP LM\n",
        "\n",
        "The Spanish model in Latin America is trained with the loaded matrix of $C_2$ of 2000 rounds and 1000 bands, noise multiplier of 8.35; the min separation is 1132 and the max participation is 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 98,
          "status": "ok",
          "timestamp": 1708225760550,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "BqFNJ1BItZ70",
        "outputId": "5bc14735-6c42-42b0-9ec2-79ee6f60cc24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DP guarantees: zcdp = 0.014342572340349278, (eps, delta) = (0.9935500950539097, 1e-10)\n"
          ]
        }
      ],
      "source": [
        "sys.setrecursionlimit(2000 * 2)  # total_rounds * max_participation\n",
        "zcdp, eps = privacy_accounting(\n",
        "    c2_matrix,\n",
        "    min_seperation=1132,\n",
        "    max_participation=2,\n",
        "    l2_clip_norm_noise_multiplier=8.35,\n",
        "    target_delta=1e-10\n",
        ")\n",
        "print(f'DP guarantees: zcdp = {zcdp}, (eps, delta) = ({eps}, 1e-10)')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
